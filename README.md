# 1C Code Bench

Бенчмарк для оценки способности LLM-моделей писать код на 1С.

## Обзор

1C Code Bench — это инструмент для бенчмаркинга, который позволяет измерить, насколько хорошо генеративные модели пишут код на 1С.
В системе содержится 20 задач, решение для которых должна написать модель.

Последовательность шагов при обчном использовании:
- Сгенерировать решения задач с помощью генератора `generate_for_bench.py`
- Замерить качество решений с помощью стартера бенчмарка `run_bench.py`

## Требования

- Python 3.11+
- Платформа 1С:Предприятие (рекомендуется 8.3.24+)
- Необходимые пакеты Python:
  - pandas
  - tqdm
  - python-dotenv

## Установка

0. Если у вас еще не становлена платформа 1С:Предприятие, установите ее согласно [официальной документации](https://online.1c.ru/catalog/free/34553751/).
1. Скачайте [выгрузку демо-базы ](https://drive.google.com/file/d/1AI_cm0HUhB-o2BpK-TeVzXfFp0kWds0b/view?usp=sharing)и разверните ее локально.
2. Клонируйте репозиторий
3. Установите необходимые пакеты:
   ```bash
   pip install -r requirements.txt
   ```
4. Создайте файл `.env` на основе `.env.example`:
   ```bash
   cp .env.example .env
   ```
5. Настройте переменные окружения в файле `.env` (см. раздел "Конфигурация")

## Конфигурация

Настройка осуществляется через файл `.env` в корне проекта. После установки скопируйте `.env.example` в `.env` и обновите следующие переменные окружения:

### Обязательные переменные:

- `DESIGNER_PATH`: Путь к исполняемому файлу конфигуратора 1С:Предприятие
  ```
  DESIGNER_PATH=<путь к папке с 1С>/bin/1cv8.exe
  ```

- `DATABASE_PATH`: Путь к вашей демо-базе, которую вы скачали и развернули на шаге 2 установки
  ```
  DATABASE_PATH=<ваш_путь_к_базе_данных>
  ```


### Дополнительные переменные:

- `PROCESSING_NAME`: Имя обработки (по умолчанию: `SampleProcessor`)
- `USER_NAME`: Имя пользователя 1С (по умолчанию: `Admin`)

**Примечание**: Файл `.env` содержит конфиденциальную информацию и не должен коммититься в репозиторий. Используйте `.env.example` как шаблон для новых установок.

## Использование

### Базовое использование

```bash
python run_bench.py <source_file> [-o OUTPUT_FILE] [--dry-run]
```

**Аргументы:**
- `source_file` — обязательный. Путь к CSV-файлу с решениями задач
- `--output`, `-o` — опционально. Путь к JSON-файлу для сохранения статистики
- `--dry-run` — опционально. Запуск в режиме валидации (выполняются эталонные решения)

**Примеры:**

```bash
# Базовый запуск
python run_bench.py data/output_sonnet-4-5.csv

# С сохранением результатов в JSON
python run_bench.py data/output_sonnet-4-5.csv -o results.json

# Пробный запуск (выполняются эталонные решения) с выводом статистики
python run_bench.py data/output_sonnet-4-5.csv --dry-run -o results.json
```

**Вывод результатов:**

После выполнения скрипт выводит в консоль:
- Общее количество задач
- Процент успешной компиляции
- Процент успешного выполнения
- Список задач с ошибками компиляции (если есть)
- Список задач с ошибками выполнения (если есть)

### Формат данных задач

Тестовые задачи должны предоставляться в виде обработки (**.epf**) со следующими функциями в модуле:
- `ваша_функция_решения()`: Код, идеально решающий задачу (для пробного запуска)
- `ЗадачаРешена()`: Код для валидации результатов
- `ЗапуститьРешение()`: подготовка данных и запуск решения

**Образец обработки для задачи бенчмарка**: `tasks\task_sample.epf` — пример структуры обработки с реализацией всех необходимых функций.

### Запуск бенчмарков

```bash
# Запуск бенчмарка с сохранением статистики
python run_bench.py my_tests.csv -o stats.json

# Режим валидации (dry-run) — проверка без реального выполнения
python run_bench.py my_tests.csv --dry-run
```
### Генерация решений задач через LLM

Скрипт `generate_for_bench.py` позволяет генерировать решения задач с помощью LLM-моделей.

**Конфигурация в `.env`:**

```bash
# Тип провайдера: "anthropic" или "openrouter"
PROVIDER_TYPE=anthropic

# Название модели (используется в имени выходного файла)
MODEL_NAME=sonnet-4-5

# Идентификатор модели
MODEL_ID=claude-sonnet-4-5-20250929

# API-ключ (в зависимости от провайдера)
ANTHROPIC_API_KEY=sk-ant-...
# или
OPENROUTER_API_KEY=sk-or-...
```

**Примеры идентификаторов моделей:**

| Провайдер | MODEL_ID |
|-----------|----------|
| Anthropic | `claude-sonnet-4-5-20250929`, `claude-3-5-sonnet-20241022` |
| OpenRouter | `anthropic/claude-3.5-sonnet`, `openai/gpt-4-turbo`, `google/gemini-pro` |

**Запуск:**

```bash
python generate_for_bench.py
```

Скрипт читает задачи из файла `data/stage_tasks3.csv` и сохраняет результаты в `data/output_{MODEL_NAME}.csv`.

**Формат входного CSV-файла:**

| Колонка | Описание |
|---------|----------|
| `task` | Текст задачи |
| `context` | Контекстная информация о структуре конфигурации 1С |

**Формат выходного CSV-файла:**

К исходным колонкам добавляется колонка `output` с сгенерированным кодом решения.

## Разработка

### Добавление новых тестовых задач

1. Сделайте клон репозитория
2. Создайте ветку для новых тестовых задач
3. Добавьте новые задачи (taks/task_*.epf) по [инструкции](https://1cbench.github.io/crowdsourcing.html).
4. Проверьте выполнение задачи локально
5. Отправьте pull request или заполните [формочку](https://docs.google.com/forms/d/e/1FAIpQLScCBd7wKty4iYDh-88Ri5-awD_yh-iw47N12z7DT5e83SZJOg/viewform)
