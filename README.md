# 1C Code Bench

Система бенчмаркинга для тестирования и оценки производительности кода 1С:Предприятие.

## Обзор

1C Code Bench — это комплексный инструмент для бенчмаркинга, предназначенный для тестирования и измерения производительности фрагментов кода 1С:Предприятие. Система автоматизирует процесс загрузки кода в базу данных 1С, его выполнения и сбора метрик производительности.

## Требования

- Python 3.11+
- Платформа 1С:Предприятие (рекомендуется 8.3.24+)
- Необходимые пакеты Python:
  - pandas
  - tqdm
  - python-dotenv

## Установка

0. Если у вас еще не становлена платформа 1С:Предприятие, установите ее согласно [официальной документации](https://online.1c.ru/catalog/free/34553751/).
1. Скачайте [выгрузку демо-базы ](https://drive.google.com/file/d/1AI_cm0HUhB-o2BpK-TeVzXfFp0kWds0b/view?usp=sharing)и разверните ее локально.
2. Клонируйте репозиторий
3. Установите необходимые пакеты:
   ```bash
   pip install -r requirements.txt
   ```
4. Создайте файл `.env` на основе `.env.example`:
   ```bash
   cp .env.example .env
   ```
5. Настройте переменные окружения в файле `.env` (см. раздел "Конфигурация")

## Конфигурация

Настройка осуществляется через файл `.env` в корне проекта. После установки скопируйте `.env.example` в `.env` и обновите следующие переменные окружения:

### Обязательные переменные:

- `DESIGNER_PATH`: Путь к исполняемому файлу конфигуратора 1С:Предприятие
  ```
  DESIGNER_PATH=<путь к папке с 1С>/bin/1cv8.exe
  ```

- `DATABASE_PATH`: Путь к вашей демо-базе, которую вы скачали и развернули на шаге 2 установки
  ```
  DATABASE_PATH=<ваш_путь_к_базе_данных>
  ```


### Дополнительные переменные:

- `PROCESSING_NAME`: Имя обработки (по умолчанию: `SampleProcessor`)
- `USER_NAME`: Имя пользователя 1С (по умолчанию: `Admin`)

**Примечание**: Файл `.env` содержит конфиденциальную информацию и не должен коммититься в репозиторий. Используйте `.env.example` как шаблон для новых установок.

## Использование

### Базовое использование

```bash
python run_bench.py <source_file> [-o OUTPUT_FILE] [--dry-run]
```

**Аргументы:**
- `source_file` — обязательный. Путь к CSV-файлу с решениями задач
- `--output`, `-o` — опционально. Путь к JSON-файлу для сохранения статистики
- `--dry-run` — опционально. Запуск в режиме валидации (выполняются эталонные решения)

**Примеры:**

```bash
# Базовый запуск
python run_bench.py data/output_sonnet-4-5.csv

# С сохранением результатов в JSON
python run_bench.py data/output_sonnet-4-5.csv -o results.json

# Пробный запуск (выполняются эталонные решения) с выводом статистики
python run_bench.py data/output_sonnet-4-5.csv --dry-run -o results.json
```

**Вывод результатов:**

После выполнения скрипт выводит в консоль:
- Общее количество образцов
- Процент успешной компиляции
- Процент успешного выполнения
- Список задач с ошибками компиляции (если есть)
- Список задач с ошибками выполнения (если есть)

### Формат данных образцов

Тестовые образцы должны предоставляться в виде обработки (**.epf**) со следующими функциями в модуле:
- `ваша_функция_решения()`: Код, идеально решающий задачу (для пробного запуска)
- `ЗадачаРешена()`: Код для валидации результатов
- `ЗапуститьРешение()`: подготовка данных и запуск решения

**Образец обработки для задачи бенчмарка**: `tasks\task_sample.epf` — пример структуры обработки с реализацией всех необходимых функций.

### Запуск бенчмарков

```bash
# Запуск бенчмарка с сохранением статистики
python run_bench.py my_tests.csv -o stats.json

# Режим валидации (dry-run) — проверка без реального выполнения
python run_bench.py my_tests.csv --dry-run
```

## Разработка

### Добавление новых тестовых задач

1. Сделайте клон репозитория
2. Создайте ветку для новых тестовых задач
3. Добавьте новые задачи (taks/task_*.epf) по [инструкции](https://1cbench.github.io/crowdsourcing.html).
4. Проверьте выполнение задачи локально
5. Отправьте pull request или заполните [формочку](https://docs.google.com/forms/d/e/1FAIpQLScCBd7wKty4iYDh-88Ri5-awD_yh-iw47N12z7DT5e83SZJOg/viewform)
